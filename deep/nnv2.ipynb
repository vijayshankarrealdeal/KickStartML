{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will implement 2 layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\r\n",
    "y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1],\n",
       "       [0, 1, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape(X.shape[0], -1).T\r\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A =  1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A,cache\n",
    "\n",
    "def back_sigmoid(dA,cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A,cache\n",
    "\n",
    "def back_relu(dA,cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z<= 0] = 0\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(n_x,n_h,n_y):\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    parameters = {\n",
    "        \"W1\":W1,\n",
    "        \"b1\":b1,\n",
    "        \"W2\":W2,\n",
    "        \"b2\":b2,\n",
    "    }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\r\n",
    "    Z = np.dot(W,A) + b\r\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\r\n",
    "    cache = (A,W,b)\r\n",
    "    return Z ,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_activation(A_prev,W,b,activation):\n",
    "    if(activation == \"relu\"):\n",
    "        Z,linear_cahe = linear_forward(A_prev,W,b)\n",
    "        A,activation_cache = relu(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        Z,linear_cahe = linear_forward(A_prev,W,b)\n",
    "        A,activation_cache = sigmoid(Z)\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cahe,activation_cache)\n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost) \n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    A_prev,W,b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ,axis= 1 ,keepdims= True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_back(dA,cache,activation):\n",
    "    linear_cache,activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = back_relu(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = back_sigmoid(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameter(parameters,grads,learning_rate):\n",
    "    parameters[\"W1\"] = parameters['W1'] - learning_rate*grads[\"dW1\"]\n",
    "    parameters[\"b1\"] = parameters['b1'] - learning_rate*grads[\"db1\"]\n",
    "    parameters[\"W2\"] = parameters['W2'] - learning_rate*grads[\"dW2\"]\n",
    "    parameters[\"b2\"] = parameters['b2'] - learning_rate*grads[\"db2\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,layer_dims,learning_rate = 0.01,num_iterations = 3000):\r\n",
    "    grads = {}\r\n",
    "    costs = []\r\n",
    "    m = X.shape[1]\r\n",
    "    print(m)\r\n",
    "    (n_x,n_h,n_y) = layer_dims\r\n",
    "    parameters = init(n_x,n_h,n_y)\r\n",
    "\r\n",
    "    W1 = parameters['W1']\r\n",
    "    b2 = parameters['b2']\r\n",
    "    W2 = parameters['W2']\r\n",
    "    b1 = parameters['b1']\r\n",
    "\r\n",
    "    for i in range(num_iterations):\r\n",
    "        A1,cache = linear_forward_activation(X,W1.T,b1,\"relu\")\r\n",
    "        A2,cache = linear_forward_activation(A1,W2.T,b2,\"sigmoid\")\r\n",
    "        cost = compute_cost(A2,Y)\r\n",
    "       \r\n",
    "        dA2 = - (np.divide(Y,A2) - np.divide(1-Y,1-A2))\r\n",
    "\r\n",
    "        dA1,dW2,db2 = linear_activation_back(dA2,cache,\"sigmoid\")\r\n",
    "        print(f'its da1 {dA1}')\r\n",
    "        dA0,dW1,db1 = linear_activation_back(dA1,cache,\"relu\")\r\n",
    "       \r\n",
    "        grads['dW1'] = dW1\r\n",
    "        grads['db1'] = db1\r\n",
    "        grads['dW2'] = dW2\r\n",
    "        grads['db2'] = db2\r\n",
    "\r\n",
    "        parameters = update_parameter(parameters,grads,learning_rate)\r\n",
    "\r\n",
    "        W1 = parameters[\"W1\"]\r\n",
    "        b1 = parameters[\"b1\"]\r\n",
    "        W2 = parameters[\"W2\"]\r\n",
    "        b2 = parameters[\"b2\"]\r\n",
    "\r\n",
    "        if i % 100 == 0:\r\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\r\n",
    "        if i % 100 == 0:\r\n",
    "            costs.append(cost)\r\n",
    "\r\n",
    "    return parameters\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = linear_forward_activation(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-54ed3297d54a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparamertes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-448a697071a3>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, Y, layer_dims, learning_rate, num_iterations)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_dims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "paramertes = model(X[0],y,[1,4,1],0.01,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d6330bcc2c4585811edcba372def02cebd4fa2d6683ed5ef2fc59fc015e52e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}